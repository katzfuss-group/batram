{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a the shrinkage version of Transport map. In this version, \n",
    "the regression functions $f_i$ and the niggest parameters $d_i$ are \n",
    "assumed to have some specific structures. These values are given by\n",
    "the parametric covariance matrix. Unlike the example in other notebook,\n",
    "here we try to estimate the parametric covariance matrix parameters \n",
    "using the integrated log-likelihood function.\n",
    "\n",
    "\n",
    "Author: Anirban Chakraborty,\n",
    "Last modified: May 13, 2024\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATAPATH = Path.cwd().parent/\"data\"\n",
    "RESULTSPATH = Path.cwd().parent/\"results\"\n",
    "PLOTSPATH = Path.cwd().parent/\"plots\"\n",
    "if not Path.is_dir(Path.cwd().parent/\"results\"):\n",
    "    Path.mkdir(Path.cwd().parent/\"results\")\n",
    "\n",
    "if not Path.is_dir(Path.cwd().parent/\"plots\"):\n",
    "    Path.mkdir(Path.cwd().parent/\"plots\")\n",
    "\n",
    "if not Path.is_dir(RESULTSPATH):\n",
    "    Path.mkdir(RESULTSPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from veccs import orderings\n",
    "from gpytorch.kernels import MaternKernel\n",
    "from sklearn.gaussian_process import kernels\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from batram.helpers import make_grid, GaussianProcessGenerator\n",
    "from batram.legmods import Data, SimpleTM\n",
    "from batram.shrinkmods import ShrinkTM, EstimableShrinkTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing log-score with the base transport maps (exponential kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20240522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kernel and location parameters\n",
    "\n",
    "num_locs = 30; dim_locs = 2\n",
    "nu_original = 0.5\n",
    "length_scale_original = 0.3\n",
    "numSamples = 30\n",
    "sd_noise=1e-6\n",
    "largest_conditioning_set = 30\n",
    "sigmasq_f = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../tests/data/NR900ExpLST30SIGSQT10.pkl\", \"rb\") as f:\n",
    "    #change the data directory if you generate data yourself \n",
    "    #by running the scripts/make_data.py file\n",
    "    data = pickle.load(f)\n",
    "locs = data[\"locs\"]\n",
    "locsorder = data[\"order\"]\n",
    "locs = locs[locsorder, :]\n",
    "gp = data[\"gp\"]\n",
    "torchdata = data[\"data\"][:, locsorder]\n",
    "nn = orderings.find_nns_l2(locs, largest_conditioning_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the data ready\n",
    "\n",
    "numSamples = [1, 2, 5, 10, 20, 30, 50, 80, 160, 200]\n",
    "reps = 10\n",
    "logScore_tm = torch.zeros((reps, len(numSamples)))\n",
    "logScore_shrink = torch.zeros((reps, len(numSamples)))\n",
    "tm_models = []\n",
    "shrink_models = []\n",
    "yreps = 50 #to be used for estimating log-score\n",
    "nsteps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit models\n",
    "for i, n in enumerate(numSamples):\n",
    "    #if (n == 1):\n",
    "    #    theta_init_fixed = torch.tensor([0.0, 0.0, -2.0, 2.0, 0.0, -0.7])\n",
    "    #else:\n",
    "    theta_init_fixed = None\n",
    "    for _reps in range(reps):\n",
    "        randperm = torch.randperm(torchdata.shape[0])\n",
    "        obs = (torchdata[randperm, :])[0:n, :] #snip first n samples\n",
    "        #if obs.dim() == 1:\n",
    "        #    obs = obs.unsqueeze(0)\n",
    "        obsTrain = obs\n",
    "        #if (n > 1):\n",
    "        #    obs = (obs - obs.mean(dim=0, keepdim=True)) / obs.std(dim=0, keepdim=True)\n",
    "\n",
    "        # Create a `Data` object for use with the `SimpleTM`/ `ShrinkTM` model.\n",
    "        data_tm = Data.new(torch.as_tensor(locs).float(), obs, torch.as_tensor(nn))\n",
    "        data_shrink = Data.new(torch.as_tensor(locs).float(), obs, torch.as_tensor(nn))\n",
    "\n",
    "        tm = SimpleTM(data_tm, theta_init=None, linear=False, smooth=1.5, nug_mult=4.0)\n",
    "        opt = torch.optim.Adam(tm.parameters(), lr=0.01)\n",
    "        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, nsteps)\n",
    "        res = tm.fit(\n",
    "            nsteps, 0.1, test_data=tm.data, optimizer=opt, scheduler=sched, batch_size=300\n",
    "        )\n",
    "        tm_models.append(tm)\n",
    "        \n",
    "        shrink_tm = EstimableShrinkTM(data=data_shrink, linear=False, \n",
    "                        transportmap_smooth=1.5, \n",
    "                        parametric_kernel= \"exponential\",\n",
    "                        param_nu=0.5,\n",
    "                        param_ls=1.0,\n",
    "                        nug_mult_bounded=False,\n",
    "                        theta_init=theta_init_fixed,\n",
    "                        )\n",
    "        \n",
    "        opt2 = torch.optim.Adam(shrink_tm.parameters(), lr=0.01)\n",
    "        sched2 = torch.optim.lr_scheduler.CosineAnnealingLR(opt2, nsteps)\n",
    "        res2 = shrink_tm.fit(\n",
    "            nsteps, 0.1, test_data=shrink_tm.data, optimizer=opt2, scheduler=sched2, batch_size=300,\n",
    "\n",
    "        )\n",
    "        shrink_models.append(shrink_tm)\n",
    "\n",
    "        testsampnum = 50\n",
    "        for _j in range(0, testsampnum):\n",
    "            with torch.no_grad():\n",
    "                logScore_tm[_reps, i] += tm.score((torchdata[randperm, :])[(200 + _j), :])/testsampnum\n",
    "            logScore_shrink[_reps, i] += shrink_tm.score((torchdata[randperm, :])[(200 + _j), :])/testsampnum\n",
    "        print(f\"n ={n}, rep {_reps} done\")\n",
    "        print(f\"tmscore = {logScore_tm[_reps, i]}, shrinkscore = {logScore_shrink[_reps, i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"gp_generator\": gp,\n",
    "    \"tm_models\": tm_models,\n",
    "    \"shrink_models\": shrink_models,\n",
    "    \"tm_logscore\" : logScore_tm,\n",
    "    \"shrink_logscore\": logScore_shrink,\n",
    "    \"numSamples\": numSamples\n",
    "}, f\"../results/modelsNR_LST{int(100*length_scale_original)}_SQT{int(100*sigmasq_f)}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrink2param",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
